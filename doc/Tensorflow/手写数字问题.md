### 前置知识基础
##### 矩阵知识
见预备数学知识目录
##### 欧式距离
欧几里得度量,又称欧式距离.是一个通常采用的距离定义,指在m维空间中两个点之间的真实距离,或者向量的自然长度.在二维,三维空间中欧式距离就是两点之间的实际距离.
$$d(x,y) = \sqrt{(x_1-y_1)^2 + (x_2-y_2)^2 + ... + (x_n-y_n)^2}$$
##### on hot编码
仅允许单一位元数字为1,其他数字都为0.在机器学习中存在one-hot向量的概念.任意维度的向量中,仅有一个维度为1,其他维度都为0.
### 手写数字问题分析
##### 目标
给定任意一个手写数字图片能够识别写的数字是多少
##### 数据来源以及处理
- 来自YanLeKun(深度学习三驾马车之一)收集的70K数据.其中60K数据用来训练,10k数据用来测试
- 例如一张写了8的图片,以及转义后的数据图片
![实际图](http://qa29ak6kj.bkt.clouddn.com//FiiGfqklZGLwbLBdlRT9yQnIcKtd)
![转义后的二维数组](http://qa29ak6kj.bkt.clouddn.com//Fsx9Sd563u7XpxE8a1CJtTf90soN)
##### 关于图片信息的转换
 >这样,就将一张图片数据转换为一个28行28列的二维数组数据.设定每个格子的值在0-256(8位一个字节二进制),0代表全白,256代表全黑.
所以我们就将一张图片数据转换为一个[28,28,1]的数据.1,代表是黑白.
如果我们拿到的是彩色图片,就需要用[28,28,3]来代替了.3代表RGB三种颜色通道

##### 关于数据打平
>对于每一张图片,我们拿到的是一个二维,甚至是三维的数据.其实这种数据是可以打平为一维的向量,计算起来会方便很多,但是会也会损失上与下的空间信息.
打平的方式是这样的:我们将第二行的数值搬到第一行的末尾,第三行..类似.最终会得到一个[28*28,1]的数据.

##### 关于分类问题的输出
>输入就是我们上文的数据,[28*28,1],输出是我们会将需要分类的每一个类别赋值一个数值信息.我们总共有0-9,共计10个数,相当于我们的输出是[10].
但是数值之间其实是有关系的,如
- 0要比1小
- 1.2要更接近于1,远离2

>实际我们要分类的类别是没有大小关系的以及中间值问题的. 1.2这种解释是并不属于我们10种问题中的任何一个类别
##### 所以我们要定义一种规则来避免上述两种问题
- 单纯的给每个类别编号,0~9,每个类别之间的关系并不带有数字的连续性关系
- 引入概率的方式,我们给出判定为某个类别的概率为多少,每个图片进行判定的时候,每个分类值都给一个概率.10个分类的概率和为1.然后认为概率最大的那个分类即我们推断分类.(one-hot)

#### Regression VS Classsification
- $y = W * X + b$ 其中y是连续的$y \in R^d$
- $out = X@W + b$ $out = [0.1, 0.8, 0.002, 0.008]$ 
>@是矩阵相乘的意思
- X: [1, 784]
- W: [784,10] 这样矩阵W才能跟矩阵X相乘
- b: [10]
- 运算过程: [1, 784]@[784, 10] + [10] = [1, 10]
##### 问题
1. 这个矩阵运算实际上还是一个线性的计算
2. 图片的识别是很复杂的,一个线性模型是很难进行识别的
##### 解决方式
1. 引入一个简单的非线性因子,ReLU函数$out = f(x@w) + b$
2. 引入火车模型.火车的特色是一节节车厢.每节车厢相当于一次处理,每次处理后的结果作为下次处理的入参.这里引入三道工序
$h_1 = relu(X @ W_1 + b_1)$
$h_2 = relu(X @ W_2 + b_2)$
$out = relu(X @ W_3 + b_3)$
##### 实际降维计算过程
###### X = [v1, v2, ..., V784] 
 - X: [1, 784]
###### h1 = relu(X@W1 + b1) 
- W1: [784, 512]
- x1: [1, 512]
###### h2 = relu(h1@W2 + b2)
- W2: [512, 256]
- b2: [256]
###### out = relu(h2@W3) + b3 
- W3: [256, 10]
- b3: [10]

![非线性因子](http://qa29ak6kj.bkt.clouddn.com//FlZPs0NXZpr-ObEinYm3HiIRM7h2)

#### loss函数(求欧式距离)
$out = relu{relu{relu[x@W_1 + b_1]@W_2 + b_2}@W_3 + b_3}$
$pred = argmax(out)$
$loss = MSE(out, lable)$

> - MSE就是求欧式距离
- lable是真实值
- 求loss的最小值过程(minimize loss)就是优化参数[w1,b1,w2,b2,w3,b3]的过程
- 得到最优[w1,b1,w2,b2,w3,b3]后,带入out式子,用来预计新的图片对应的手写数字

#### 如果我们不是做三道工序,而是30道工序,那么就是deep learning了

### 实际编码
```
import os
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import datasets, layers, optimizers

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# 在线加载mnist数据
# 加载的数据是numpy的格式
# x是数据,y是实际结果
(x, y), (x_val, y_val) = datasets.mnist.load_data()
# 将numpy格式数据转换为tensor格式的数据
x = tf.convert_to_tensor(x, dtype=tf.float32) / 255.
y = tf.convert_to_tensor(y, dtype=tf.int32)
y = tf.one_hot(y, depth=10)
print('datasets:', x.shape, y.shape)
# 将给定数据集进行分片处理
train_dataset = tf.data.Dataset.from_tensor_slices((x, y))
# 设定分片批次大小
train_dataset = train_dataset.batch(200)

# 准备阶段,准备网络结构 这里是设定了一个3层的网络结构 784 -> 512 512 -> 256 256 -> 10
model = keras.Sequential([
    layers.Dense(512, activation='relu'),
    layers.Dense(256, activation='relu'),
    layers.Dense(10)])
# 准备优化器 构造一个随机的梯度下降算法,并初始化一个给定的偏移率
optimizer = optimizers.SGD(learning_rate=0.001)


def train_epoch(epoch):
    # print("=========================================")
    # step4. loop
    # train_dataset 已在上文设为每次训练使用200个图片数据
    for step, (x, y) in enumerate(train_dataset):

        with tf.GradientTape() as tape:
            # [b, 28, 28] -> [b, 784] 将二维数据打平到一维
            x = tf.reshape(x, (-1, 28 * 28))
            # step 1. compute output 进行深度计算
            # [b, 784] -> [b, 10]
            out = model(x)
            # step2. compute loss .
            loss = tf.reduce_sum(tf.square(out - y)) / x.shape[0]
        # step3, 自动求导 optimize and update w1, w2, w3, b1, b2, b3
        grads = tape.gradient(loss, model.trainable_variables)
        # w_2 = w_1 - lr * grade
        optimizer.apply_gradients(zip(grads, model.trainable_variables))

        if step % 99 == 0:
            print("epoch:", epoch, step, 'loss:', loss.numpy)


def train():
    # 将程序使用同一个数据集训练30次
    print("=====start=====")
    for epoch in range(30):
        train_epoch(epoch)
        print("=======================train epoch = {0}=======================", epoch)
    print("=====end=====")


if __name__ == '__main__':
    train()
```
#### 运行结果
```
第一次训练:
epoch: 0 0 loss: <bound method _EagerTensorBase.numpy of <tf.Tensor: id=107, shape=(), dtype=float32, numpy=1.9502934>>
epoch: 0 99 loss: <bound method _EagerTensorBase.numpy of <tf.Tensor: id=5878, shape=(), dtype=float32, numpy=1.0012333>>
epoch: 0 198 loss: <bound method _EagerTensorBase.numpy of <tf.Tensor: id=11620, shape=(), dtype=float32, numpy=0.828062>>
epoch: 0 297 loss: <bound method _EagerTensorBase.numpy of <tf.Tensor: id=17362, shape=(), dtype=float32, numpy=0.6931744>>
第30次训练:
epoch: 29 0 loss: <bound method _EagerTensorBase.numpy of <tf.Tensor: id=504881, shape=(), dtype=float32, numpy=0.2599083>>
epoch: 29 99 loss: <bound method _EagerTensorBase.numpy of <tf.Tensor: id=510623, shape=(), dtype=float32, numpy=0.27221897>>
epoch: 29 198 loss: <bound method _EagerTensorBase.numpy of <tf.Tensor: id=516365, shape=(), dtype=float32, numpy=0.30807847>>
epoch: 29 297 loss: <bound method _EagerTensorBase.numpy of <tf.Tensor: id=522107, shape=(), dtype=float32, numpy=0.20265716>>
```
>##### 由日志可知,经过30次训练后,我们的误差函数的结果由最开始1.9502934变为最后的0.20265716,基本来说下降了10倍
##### 要注意的是,进行深度学习的训练对硬件要求是很高的,我R7的CPU基本用了80%